{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pl_bolts\n",
    "#from pl_bolts.models.vision import UNet\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import torchmetrics as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about pre and post processing with computer vision approaches.\n",
    "E.g.  \n",
    "For Preprocessing: Use feature extraction like line detection as potential features into the network.  \n",
    "For Postprocessing: Define clear edges and connect segments if enough is predicted to be a road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSatelliteModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size: int = 16, num_workers: int = 8):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.train_images = self.read_images('cil_data/training/training/images/', ImageReadMode.RGB)\n",
    "        self.train_masks = self.read_images('cil_data/training/training/groundtruth/', ImageReadMode.GRAY)\n",
    "    \n",
    "        for i, train_mask in enumerate(self.train_masks):\n",
    "            self.train_masks[i][self.train_masks[i] > 0] = 1  # 255\n",
    "            \n",
    "        # self.train_images_float = [convert_image_dtype(img, dtype=torch.float) for img in self.train_images]\n",
    "        # self.train_masks_float = [convert_image_dtype(img, dtype=torch.float) for img in self.train_masks]\n",
    "            \n",
    "        self.train_zip = list(zip(self.train_images, self.train_masks)) # _float\n",
    "        \n",
    "        test_files = [(x[0], x[2]) for x in os.walk('cil_data/test_images/test_images/')]\n",
    "        self.file_names = test_files[0][1]\n",
    "        self.test_images = list()\n",
    "        for file_name in self.file_names:\n",
    "            self.test_images.append(read_image(str('cil_data/test_images/test_images/' + file_name), ImageReadMode.RGB))\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage in (None, 'fit'):\n",
    "            train_length = int(len(self.train_zip) * 0.8)\n",
    "            valid_length = len(self.train_zip) - train_length\n",
    "\n",
    "            self.train_data, self.valid_data = random_split(self.train_zip, [train_length, valid_length])\n",
    "            \n",
    "        if stage in (None, 'test'):\n",
    "            self.test_data = list(zip(self.test_images, self.file_names))\n",
    "            \n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_data, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_data, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def read_images(self, data_dir, read_mode):\n",
    "        return [read_image(data_dir + file, read_mode) for file in os.listdir(data_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_data = RoadSatelliteModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_data.prepare_data()\n",
    "road_data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_images = [draw_segmentation_masks(\n",
    "                image=train_pair[0], masks=train_pair[1].bool(),\n",
    "                alpha=0.6,\n",
    "                colors=[\"#00FF00\"]\n",
    "              ) \n",
    "              for train_pair in road_data.train_zip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(seg_images[69])\n",
    "\n",
    "#for seg_image in seg_images:\n",
    "#    show_image(seg_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = next(iter(road_data.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tx: \", tx.shape)\n",
    "print(\"ty: \", len(ty))\n",
    "# print(\"Test Image \", ty[0], \" has tensor:\\n\", tx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationSystem(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module, datamodule: pl.LightningDataModule, lr: float = 2e-4, batch_size: int = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.datamodule = datamodule\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        \n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        \n",
    "        y_pred = self.model(X)\n",
    "       \n",
    "        loss = sigmoid_focal_loss(y_pred, y, reduction='mean')\n",
    "        \n",
    "        self.log('training_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        \n",
    "        X = X.float()\n",
    "        y = y.int()\n",
    "        \n",
    "        y_pred = self.model(X)\n",
    "        y_sig = torch.sigmoid(y_pred)\n",
    "       \n",
    "        metric = tm.functional.accuracy(y_sig, y)\n",
    "        \n",
    "        self.log('validation_metric', metric)\n",
    "        \n",
    "        return metric\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, name = batch\n",
    "        \n",
    "        X = X.float()\n",
    "        \n",
    "        return (self.model(X), name)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.test_results = outputs\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return self.datamodule.train_dataloader()\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.datamodule.val_dataloader()\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.datamodule.test_dataloader()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, verbose=2)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'validation_metric'\n",
    "        }\n",
    "    \n",
    "    def visualize_results(self):\n",
    "            Xs, ys = next(iter(self.val_dataloader()))\n",
    "            \n",
    "            y_preds = torch.sigmoid(self.model(Xs.float()))\n",
    "            \n",
    "            for y_pred in y_preds:\n",
    "                show_image(y_pred)\n",
    "\n",
    "    def visualize_results_overlay(self):\n",
    "        Xs, ys = next(iter(self.val_dataloader()))\n",
    "        \n",
    "        y_preds = torch.sigmoid(self.model(Xs.float()))\n",
    "        \n",
    "        pred_zip = list(zip(Xs, y_preds))\n",
    "        \n",
    "        seg_images = [draw_segmentation_masks(train_pair[0], train_pair[1].round().bool(), alpha=0.6, colors=['#00ff00']) for train_pair in pred_zip]\n",
    "        \n",
    "        for seg_image in seg_images:\n",
    "            show_image(seg_image)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 1, 5, padding='same')\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.float()\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = UNet(1, 3, 5, 12)  #UNet form pytorch-lightning-bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatially dependent (nested) u-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unet implementation: \n",
    "#     https://idiotdeveloper.com/unet-implementation-in-pytorch/\n",
    "#     https://github.com/4uiiurz1/pytorch-nested-unet/blob/master/archs.py\n",
    "# augment with spatial layers: \n",
    "#     https://github.com/djordjemila/sdn/blob/main/lib/nn.py\n",
    "# augment with dialation: \n",
    "#     https://arxiv.org/pdf/2004.03466.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDNCell(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.gru = GRUCell(input_size=3*num_features, hidden_size=num_features)\n",
    "\n",
    "    def forward(self, neighboring_features, features):\n",
    "        \"\"\" Update current features based on neighboring features \"\"\"\n",
    "        return self.gru(torch.cat(neighboring_features, dim=1), features)\n",
    "\n",
    "\n",
    "class _CorrectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, dir=0):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.cell = SDNCell(num_features)\n",
    "        if dir == 0:\n",
    "            self.forward = self.forward0\n",
    "        elif dir == 1:\n",
    "            self.forward = self.forward1\n",
    "        elif dir == 2:\n",
    "            self.forward = self.forward2\n",
    "        else:\n",
    "            self.forward = self.forward3\n",
    "        self.zero_pad = None\n",
    "\n",
    "    def _get_zero_pad(self, batch, device):\n",
    "        if self.zero_pad is None or self.zero_pad.shape[0] != batch:\n",
    "            self.zero_pad = torch.zeros((batch, self.num_features, 1), device=device)  # no grad ??\n",
    "        return self.zero_pad\n",
    "\n",
    "    def forward0(self, features):\n",
    "        batch = features.shape[0]\n",
    "        dim = features.shape[2]\n",
    "        zero_pad = self._get_zero_pad(batch, features.device)\n",
    "        # make a loop\n",
    "        for d in range(1, dim):\n",
    "            neighboring_features = torch.cat([zero_pad, features[:, :, :, d - 1], zero_pad], dim=2).transpose(1, 2)\n",
    "            # compute features\n",
    "            features[:, :, :, d] = self.cell(\n",
    "                neighboring_features=[neighboring_features[:, :-2, :].reshape(-1, self.num_features),\n",
    "                                      neighboring_features[:, 1:-1, :].reshape(-1, self.num_features),\n",
    "                                      neighboring_features[:, 2:, :].reshape(-1, self.num_features)],\n",
    "                features=features[:, :, :, d].transpose(1, 2).reshape(-1, self.num_features).clone()\n",
    "            ).reshape(batch, -1, self.num_features).transpose(1, 2)\n",
    "        # return new features\n",
    "        return features\n",
    "\n",
    "    def forward1(self, features):\n",
    "        batch = features.shape[0]\n",
    "        dim = features.shape[2]\n",
    "        zero_pad = self._get_zero_pad(batch, features.device)\n",
    "        # make a loop\n",
    "        for d in range(dim - 2, -1, -1):\n",
    "            neighboring_features = torch.cat([zero_pad, features[:, :, :, d + 1], zero_pad], dim=2).transpose(1, 2)\n",
    "            # compute features\n",
    "            features[:, :, :, d] = self.cell(\n",
    "                neighboring_features=[neighboring_features[:, :-2, :].reshape(-1, self.num_features),\n",
    "                                      neighboring_features[:, 1:-1, :].reshape(-1, self.num_features),\n",
    "                                      neighboring_features[:, 2:, :].reshape(-1, self.num_features)],\n",
    "                features=features[:, :, :, d].transpose(1, 2).reshape(-1, self.num_features).clone()\n",
    "            ).reshape(batch, -1, self.num_features).transpose(1, 2)\n",
    "        # return new features\n",
    "        return features\n",
    "\n",
    "    def forward2(self, features):\n",
    "        batch = features.shape[0]\n",
    "        dim = features.shape[2]\n",
    "        zero_pad = self._get_zero_pad(batch, features.device)\n",
    "        # make a loop\n",
    "        for d in range(1, dim):\n",
    "            neighboring_features = torch.cat([zero_pad, features[:, :, d - 1, :], zero_pad], dim=2).transpose(1, 2)\n",
    "            # compute features\n",
    "            features[:, :, d, :] = self.cell(\n",
    "                neighboring_features=[neighboring_features[:, :-2, :].reshape(-1, self.num_features),\n",
    "                                      neighboring_features[:, 1:-1, :].reshape(-1, self.num_features),\n",
    "                                      neighboring_features[:, 2:, :].reshape(-1, self.num_features)],\n",
    "                features=features[:, :, d, :].transpose(1, 2).reshape(-1, self.num_features).clone()\n",
    "            ).reshape(batch, -1, self.num_features).transpose(1, 2)\n",
    "        # return new features\n",
    "        return features\n",
    "\n",
    "    def forward3(self, features):\n",
    "        batch = features.shape[0]\n",
    "        dim = features.shape[2]\n",
    "        zero_pad = self._get_zero_pad(batch, features.device)\n",
    "        # make a loop\n",
    "        for d in range(dim - 2, -1, -1):\n",
    "            neighboring_features = torch.cat([zero_pad, features[:, :, d + 1, :], zero_pad], dim=2).transpose(1, 2)\n",
    "            # compute features\n",
    "            features[:, :, d, :] = self.cell(\n",
    "                neighboring_features=[neighboring_features[:, :-2, :].reshape(-1, self.num_features),\n",
    "                                      neighboring_features[:, 1:-1, :].reshape(-1, self.num_features),\n",
    "                                      neighboring_features[:, 2:, :].reshape(-1, self.num_features)],\n",
    "                features=features[:, :, d, :].transpose(1, 2).reshape(-1, self.num_features).clone()\n",
    "            ).reshape(batch, -1, self.num_features).transpose(1, 2)\n",
    "        # return new features\n",
    "        return features\n",
    "\n",
    "\n",
    "class SDNLayer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, num_features, dirs, kernel_size, stride, padding, upsample):\n",
    "        super().__init__()\n",
    "        # project-in network\n",
    "        cnn_module = nn.ConvTranspose2d if upsample else nn.Conv2d\n",
    "        self.project_in_stage = cnn_module(in_ch, num_features, kernel_size, stride, padding)\n",
    "        # correction network\n",
    "        sdn_correction_layers = []\n",
    "        for dir in dirs:\n",
    "            sdn_correction_layers.append(_CorrectionLayer(num_features, dir=dir))\n",
    "        self.sdn_correction_stage = nn.Sequential(*sdn_correction_layers)\n",
    "        # project-out network\n",
    "        self.project_out_stage = nn.Conv2d(num_features, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (I) project-in stage\n",
    "        x = self.project_in_stage(x)\n",
    "        x = torch.tanh(x)\n",
    "        # (II) correction stage\n",
    "        x = x.contiguous(memory_format=torch.channels_last)\n",
    "        x = self.sdn_correction_stage(x)\n",
    "        x = x.contiguous(memory_format=torch.contiguous_format)\n",
    "        # (III) project-out stage\n",
    "        x = self.project_out_stage(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResSDNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, num_features, dirs, kernel_size, stride, padding, upsample):\n",
    "        super().__init__()\n",
    "        self.sdn = SDNLayer(in_ch, 2 * out_ch, num_features, dirs, kernel_size, stride, padding, upsample)\n",
    "        cnn_module = nn.ConvTranspose2d if upsample else nn.Conv2d\n",
    "        self.cnn = cnn_module(in_ch, out_ch, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, input):\n",
    "        cnn_out = self.cnn(input)\n",
    "        sdn_out, gate = self.sdn(input).chunk(2, 1)\n",
    "        gate = torch.sigmoid(gate)\n",
    "        return gate * cnn_out + (1-gate) * sdn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        #nb_filter = [32, 64, 128, 256, 512]\n",
    "        nb_filter = [32, 64, 256]\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = conv_block(input_channels, nb_filter[0])\n",
    "        self.conv1_0 = conv_block(nb_filter[0], nb_filter[1])\n",
    "        self.conv2_0 = conv_block(nb_filter[1], nb_filter[2])\n",
    "        #self.conv3_0 = conv_block(nb_filter[2], nb_filter[3])\n",
    "        #self.conv4_0 = conv_block(nb_filter[3], nb_filter[4])\n",
    "\n",
    "        #self.conv3_1 = conv_block(nb_filter[3]+nb_filter[4], nb_filter[3])\n",
    "        #self.conv2_2 = conv_block(nb_filter[2]+nb_filter[3], nb_filter[2])\n",
    "        self.conv1_3 = conv_block(nb_filter[1]+nb_filter[2], nb_filter[1])\n",
    "        self.conv0_4 = conv_block(nb_filter[0]+nb_filter[1], nb_filter[0])\n",
    "\n",
    "        self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        #x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        #x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "\n",
    "        \n",
    "        \n",
    "        #x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
    "        #x2_2 = self.conv2_2(torch.cat([x2_0, self.up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, self.up(x2_0)], 1))  #x2_2\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, self.up(x1_3)], 1))\n",
    "\n",
    "        output = self.final(x0_4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedUNet(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, deep_supervision=False, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # depth = 4\n",
    "        # use loop instead of defining all the convs individually\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = conv_block(input_channels, nb_filter[0])\n",
    "        self.conv1_0 = conv_block(nb_filter[0], nb_filter[1])\n",
    "        self.conv2_0 = conv_block(nb_filter[1], nb_filter[2])\n",
    "        self.conv3_0 = conv_block(nb_filter[2], nb_filter[3])\n",
    "        self.conv4_0 = conv_block(nb_filter[3], nb_filter[4])\n",
    "\n",
    "        self.conv0_1 = conv_block(nb_filter[0]+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_1 = conv_block(nb_filter[1]+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_1 = conv_block(nb_filter[2]+nb_filter[3], nb_filter[2])\n",
    "        self.conv3_1 = conv_block(nb_filter[3]+nb_filter[4], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = conv_block(nb_filter[0]*2+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_2 = conv_block(nb_filter[1]*2+nb_filter[2], nb_filter[1])\n",
    "        self.conv2_2 = conv_block(nb_filter[2]*2+nb_filter[3], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = conv_block(nb_filter[0]*3+nb_filter[1], nb_filter[0])\n",
    "        self.conv1_3 = conv_block(nb_filter[1]*3+nb_filter[2], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = conv_block(nb_filter[0]*4+nb_filter[1], nb_filter[0])\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            output1 = self.final1(x0_1)\n",
    "            output2 = self.final2(x0_2)\n",
    "            output3 = self.final3(x0_3)\n",
    "            output4 = self.final4(x0_4)\n",
    "            return [output1, output2, output3, output4]\n",
    "\n",
    "        else:\n",
    "            output = self.final(x0_4)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SemanticSegmentationSystem(model, road_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inspect System and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(system.batch_size, 3, 400, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Shapes and Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(road_data.train_dataloader()))\n",
    "print(\"Shapes:\\nX: \", X.shape, \"\\ny: \", y.shape)\n",
    "print(\"Types:\\nX type: \", X.type(), \"\\ny type: \", y.type())\n",
    "print(\"Datatypes:\\nX dtype: \", X[0].dtype, \"\\ny dtype: \", y[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X.float())\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.training_step((X, y), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.validation_step((X, y), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system.visualize_results_overlay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system.visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='validation_metric',\n",
    "   patience=5,\n",
    "   verbose=2,\n",
    "   mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_count = -1\n",
    "    print(\"GPUs detected.\")\n",
    "    print(\"There should be \", torch.cuda.device_count(), \" GPUs available.\")\n",
    "else:\n",
    "    gpu_count = 0\n",
    "    print(\"No GPU detected.\")\n",
    "    print(\"Working with CPU\")\n",
    "\n",
    "gpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    #fast_dev_run=True,\n",
    "    #max_epochs = 1,  # at least 1 requried. Only for testing the prediction\n",
    "    gpus=gpu_count,\n",
    "    auto_lr_find=True,\n",
    "    auto_scale_batch_size='binsearch',\n",
    "    stochastic_weight_avg=True,\n",
    "    logger=False,\n",
    "    deterministic=True,\n",
    "    callbacks=[early_stop_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.tune(system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Predict Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = system.test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(im, name):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", name).group(0))\n",
    "    #im = mpimg.imread(image_filename) \n",
    "    # image is gray scale therefore size MxN with imread \n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i:i + patch_size, j:j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *images):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for imgs, fn in images[0:]:\n",
    "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(imgs, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filename = \"some_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_counter = 0\n",
    "\n",
    "with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        # iterate over batches and get the ids and predictions\n",
    "        for masks, names in batches:\n",
    "            for i, mask in enumerate(masks):\n",
    "                #print(\"mask \", i, \" is: \", names[i])\n",
    "                predicted_mask = np.asarray(torch.sigmoid(mask).squeeze())\n",
    "\n",
    "                ids = mask_to_submission_strings(predicted_mask, names[i])\n",
    "                # print('\\n'.join(id))\n",
    "                f.writelines('{}\\n'.format('\\n'.join(ids)))\n",
    "\n",
    "                pred_counter += 1\n",
    "pred_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.visualize_results_overlay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scratch Area to try out things at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Spatial Depenent Layers\n",
    "        \n",
    "        # Convolutional Layer\n",
    "        # also try dialated for longer reach \n",
    "        \n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.float()\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ikernel_cil",
   "language": "python",
   "name": "ikernel_cil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
